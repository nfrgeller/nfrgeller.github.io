---
draft: true
execute:
  echo: true
  eval: true
  warning: true
  error: true 
categories:
  - ML
  - Data Science 
title: "Gradient Boosting"
author: "Noah Geller"
date: "10/30/24"
---

The standard setup of a predictive machine learning problem consists of: 

  - An outcome variable $y\in \mathcal{Y}$ that you want to predict.

  - A collection of features $X = x_1,\dots,x_p \in \mathcal{X}$ which you believe can help you predict $y$. 

  - A finite data set of pairs $\{(X_i,y_i)\}_{i=1}^n\subset \mathcal{X}\times\mathcal{Y}$.

  - A hypothesis class of predictive functions $\mathcal{F}\subset \mathcal{Y}^\mathcal{X}$. 
  
  - A loss function $L:\mathcal{F} \times \mathcal{X}\times \mathcal{Y} \to \mathbb{R}$ which describes how poorly a candidate predictor $f\in \mathcal{F}$ performs on a known pair $(X,y)$. 

The goal is then to find the predictor $f\in \mathcal{F}$ which minimizes the expected loss over all possible $(X,y)$ pairs. We only have access to a finite sample from the true joint distribution of $X$ and $y$, so we usually define 
$$\mathscr{L}(f) = \frac{1}{n}\sum_{i=1}^n L(f,X_i,y_i)$$
to be the average loss of a predictor over the whole training dataset and seek to minimize this quantity as a proxy for minmizing the expected loss over the true distribution.  

The most straightforward way to make this problem computationally tractable is to choose a hypothesis class $\mathcal{F}$ which can be parameterized by a finite dimensional Euclidean space and a loss function $L$ which is differentiable with respect to those parameters. This allows us to exploit the miraculous power of (stochastic) gradient descent to find a vector of parameters which (locally) minimizes $\mathscr{L}$. Although we usually think of gradient descent as just an iterative algorithm that continually updates our candidate vector of parameters, we can also remember our previous steps and write the $k$'th iteration as the sum

\begin{align*}
\theta_k &= \theta_0 - \rho_1 \nabla_\theta \mathscr{L}(f_{\theta_0}) - \rho_2 \nabla_\theta \mathscr{L}(f_{\theta_1}) - \dots \\
&= \theta_0 + \sum_{i=1}^k -\rho_k \nabla_\theta \mathscr{L}(f_{\theta_{k-1}})
\end{align*}
where $\rho_i$ is the $i$'th step size (perhaps obtained by doing a line search) and $f_{\theta_i}$ is the predictive function corresponding to the parameters $\theta_i$. 

Here we are starting with an initial guess for the parameters, $\theta_0$, and then iteratively adding correction terms to this initial guess in order to make the loss smaller and smaller. One compromise that we are making in this process is that we are using the vector space of parameters as a proxy for the vector space of functions in our hypothesis class. For instance, it is not true in general that $$f_{\theta_1+\theta_2} = f_{\theta_1}+f_{\theta_2}.$$

The idea behind boosting is to do a similar procedure, but where we start with an intial guess for a predictive function and then iteratively add on corrective terms which are themselves functions $f:\mathcal{X}\to \mathcal{Y}$. 

Suppose we start with an initial predictor $f(X) = h_0(X)$. We want to choose our next term $h_1(X)$ so that 
$$\mathscr{L}(h_0+h_1) < \mathscr{L}(h_0).$$




