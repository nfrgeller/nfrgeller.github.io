{"title":"Gradient Boosting","markdown":{"yaml":{"execute":{"echo":true,"eval":true,"warning":true,"error":true},"categories":["ML","Data Science"],"title":"Gradient Boosting","author":"Noah Geller","date":"10/30/24"},"containsRefs":false,"markdown":"\n\nThe standard setup of a predictive machine learning problem consists of: \n\n  - An outcome variable $y\\in \\mathcal{Y}$ that you want to predict.\n\n  - A collection of features $X = x_1,\\dots,x_p \\in \\mathcal{X}$ which you believe can help you predict $y$. \n\n  - A finite data set of pairs $\\{(X_i,y_i)\\}_{i=1}^n\\subset \\mathcal{X}\\times\\mathcal{Y}$.\n\n  - A hypothesis class of predictive functions $\\mathcal{F}\\subset \\mathcal{Y}^\\mathcal{X}$. \n  \n  - A loss function $L:\\mathcal{F} \\times \\mathcal{X}\\times \\mathcal{Y} \\to \\mathbb{R}$ which describes how poorly a candidate predictor $f\\in \\mathcal{F}$ performs on a known pair $(X,y)$. \n\nThe goal is then to find the predictor $f\\in \\mathcal{F}$ which minimizes the expected loss over all possible $(X,y)$ pairs. We only have access to a finite sample from the true joint distribution of $X$ and $y$, so we usually define \n$$\\mathscr{L}(f) = \\frac{1}{n}\\sum_{i=1}^n L(f,X_i,y_i)$$\nto be the average loss of a predictor over the whole training dataset and seek to minimize this quantity as a proxy for minmizing the expected loss over the true distribution.  \n\nThe most straightforward way to make this problem computationally tractable is to choose a hypothesis class $\\mathcal{F}$ which can be parameterized by a finite dimensional Euclidean space and a loss function $L$ which is differentiable with respect to those parameters. This allows us to exploit the miraculous power of (stochastic) gradient descent to find a vector of parameters which (locally) minimizes $\\mathscr{L}$. Although we usually think of gradient descent as just an iterative algorithm that continually updates our candidate vector of parameters, we can also remember our previous steps and write the $k$'th iteration as the sum\n\n\\begin{align*}\n\\theta_k &= \\theta_0 - \\rho_1 \\nabla_\\theta \\mathscr{L}(f_{\\theta_0}) - \\rho_2 \\nabla_\\theta \\mathscr{L}(f_{\\theta_1}) - \\dots \\\\\n&= \\theta_0 + \\sum_{i=1}^k -\\rho_k \\nabla_\\theta \\mathscr{L}(f_{\\theta_{k-1}})\n\\end{align*}\nwhere $\\rho_i$ is the $i$'th step size (perhaps obtained by doing a line search) and $f_{\\theta_i}$ is the predictive function corresponding to the parameters $\\theta_i$. \n\nHere we are starting with an initial guess for the parameters, $\\theta_0$, and then iteratively adding correction terms to this initial guess in order to make the loss smaller and smaller. One compromise that we are making in this process is that we are using the vector space of parameters as a proxy for the vector space of functions in our hypothesis class. For instance, it is not true in general that $$f_{\\theta_1+\\theta_2} = f_{\\theta_1}+f_{\\theta_2}.$$\n\nThe idea behind boosting is to do a similar procedure, but where we start with an intial guess for a predictive function and then iteratively add on corrective terms which are themselves functions $f:\\mathcal{X}\\to \\mathcal{Y}$. \n\nSuppose we start with an initial predictor $f(X) = h_0(X)$. We want to choose our next term $h_1(X)$ so that \n$$\\mathscr{L}(h_0+h_1) < \\mathscr{L}(h_0).$$\n\n\n\n\n","srcMarkdownNoYaml":"\n\nThe standard setup of a predictive machine learning problem consists of: \n\n  - An outcome variable $y\\in \\mathcal{Y}$ that you want to predict.\n\n  - A collection of features $X = x_1,\\dots,x_p \\in \\mathcal{X}$ which you believe can help you predict $y$. \n\n  - A finite data set of pairs $\\{(X_i,y_i)\\}_{i=1}^n\\subset \\mathcal{X}\\times\\mathcal{Y}$.\n\n  - A hypothesis class of predictive functions $\\mathcal{F}\\subset \\mathcal{Y}^\\mathcal{X}$. \n  \n  - A loss function $L:\\mathcal{F} \\times \\mathcal{X}\\times \\mathcal{Y} \\to \\mathbb{R}$ which describes how poorly a candidate predictor $f\\in \\mathcal{F}$ performs on a known pair $(X,y)$. \n\nThe goal is then to find the predictor $f\\in \\mathcal{F}$ which minimizes the expected loss over all possible $(X,y)$ pairs. We only have access to a finite sample from the true joint distribution of $X$ and $y$, so we usually define \n$$\\mathscr{L}(f) = \\frac{1}{n}\\sum_{i=1}^n L(f,X_i,y_i)$$\nto be the average loss of a predictor over the whole training dataset and seek to minimize this quantity as a proxy for minmizing the expected loss over the true distribution.  \n\nThe most straightforward way to make this problem computationally tractable is to choose a hypothesis class $\\mathcal{F}$ which can be parameterized by a finite dimensional Euclidean space and a loss function $L$ which is differentiable with respect to those parameters. This allows us to exploit the miraculous power of (stochastic) gradient descent to find a vector of parameters which (locally) minimizes $\\mathscr{L}$. Although we usually think of gradient descent as just an iterative algorithm that continually updates our candidate vector of parameters, we can also remember our previous steps and write the $k$'th iteration as the sum\n\n\\begin{align*}\n\\theta_k &= \\theta_0 - \\rho_1 \\nabla_\\theta \\mathscr{L}(f_{\\theta_0}) - \\rho_2 \\nabla_\\theta \\mathscr{L}(f_{\\theta_1}) - \\dots \\\\\n&= \\theta_0 + \\sum_{i=1}^k -\\rho_k \\nabla_\\theta \\mathscr{L}(f_{\\theta_{k-1}})\n\\end{align*}\nwhere $\\rho_i$ is the $i$'th step size (perhaps obtained by doing a line search) and $f_{\\theta_i}$ is the predictive function corresponding to the parameters $\\theta_i$. \n\nHere we are starting with an initial guess for the parameters, $\\theta_0$, and then iteratively adding correction terms to this initial guess in order to make the loss smaller and smaller. One compromise that we are making in this process is that we are using the vector space of parameters as a proxy for the vector space of functions in our hypothesis class. For instance, it is not true in general that $$f_{\\theta_1+\\theta_2} = f_{\\theta_1}+f_{\\theta_2}.$$\n\nThe idea behind boosting is to do a similar procedure, but where we start with an intial guess for a predictive function and then iteratively add on corrective terms which are themselves functions $f:\\mathcal{X}\\to \\mathcal{Y}$. \n\nSuppose we start with an initial predictor $f(X) = h_0(X)$. We want to choose our next term $h_1(X)$ so that \n$$\\mathscr{L}(h_0+h_1) < \\mathscr{L}(h_0).$$\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"boosting.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","grid":{"body-width":"1100px"},"theme":["flatly","../theme-dark.scss"],"mainfont":"Noto Sans","fontsize":"25px","toc-location":"left-body","toc-title":"Contents","toc-expand":2,"code-copy":true,"categories":["ML","Data Science"],"title":"Gradient Boosting","author":"Noah Geller","date":"10/30/24"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}